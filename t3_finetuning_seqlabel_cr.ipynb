{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/limshaocong/SysBERT/blob/main/t3_finetuning_seqlabel_cr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preliminaries**"
      ],
      "metadata": {
        "id": "b0LYLPA78zuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "! pip install --user datasets transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG-L9BEI8uie",
        "outputId": "3702f5ff-511e-4c4a-912c-724bb69d3ea4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-694f0c32-a1d1-c639-5e2e-f66268b8b899)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! huggingface-cli login\n",
        "# # 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"
      ],
      "metadata": {
        "id": "WPpnEQBc8vx7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "# **Import & Pre-process Data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_type_dict = {\n",
        "    'bert-base-cased' : 'bert-base-cased',\n",
        "    'roberta-base' : 'roberta-base',\n",
        "    'allenai/scibert_scivocab_cased' : 'allenai/scibert_scivocab_cased',\n",
        "    'limsc/reqbert-tapt-epoch29' : 'bert-base-cased', # preferred\n",
        "    'limsc/reqbert-tapt-epoch30' : 'bert-base-cased',\n",
        "    'limsc/reqroberta-tapt-epoch20' : 'roberta-base',\n",
        "    'limsc/reqroberta-tapt-epoch33' : 'roberta-base',\n",
        "    'limsc/reqroberta-tapt-epoch43' : 'roberta-base', # preferred\n",
        "    'limsc/reqroberta-tapt-epoch50' : 'roberta-base',\n",
        "    'limsc/reqscibert-tapt-epoch10' : 'allenai/scibert_scivocab_cased', # preferred\n",
        "    'limsc/reqscibert-tapt-epoch20' : 'allenai/scibert_scivocab_cased', # preferred\n",
        "    'limsc/reqscibert-tapt-epoch31' : 'allenai/scibert_scivocab_cased',\n",
        "    'limsc/reqscibert-tapt-epoch49' : 'allenai/scibert_scivocab_cased',\n",
        "}\n",
        "\n",
        "model_name_dict = {\n",
        "    'bert-base-cased' : 'bert',\n",
        "    'roberta-base' : 'roberta',\n",
        "    'allenai/scibert_scivocab_cased' : 'scibert',\n",
        "    'limsc/reqbert-tapt-epoch29' : 'reqbert-e29',\n",
        "    'limsc/reqbert-tapt-epoch30' : 'reqbert-e30',\n",
        "    'limsc/reqroberta-tapt-epoch20' : 'reqroberta-e20',\n",
        "    'limsc/reqroberta-tapt-epoch33' : 'reqroberta-e33',\n",
        "    'limsc/reqroberta-tapt-epoch43' : 'reqroberta-e43',\n",
        "    'limsc/reqroberta-tapt-epoch50' : 'reqroberta-e50',\n",
        "    'limsc/reqscibert-tapt-epoch10' : 'reqscibert-e10',\n",
        "    'limsc/reqscibert-tapt-epoch20' : 'reqscibert-e20',\n",
        "    'limsc/reqscibert-tapt-epoch31' : 'reqscibert-e31',\n",
        "    'limsc/reqscibert-tapt-epoch49' : 'reqscibert-e49',\n",
        "}\n",
        "\n",
        "task_name_dict = {\n",
        "    'limsc/fr-nfr-classification' : 'frnfr',\n",
        "    'limsc/nfr-subclass-classification' : 'subclass',\n",
        "    'limsc/concept-recognition' : 'cr',\n",
        "    'limsc/concept-recognition-not-iob' : 'cr',\n",
        "    'limsc/sysmlv2-entity-extraction' : 'ee'\n",
        "}"
      ],
      "metadata": {
        "id": "HNVQoJ1CKvSb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IreSlFmlIrIm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "45eca55ace8445cd95985153d1282f51",
            "9224affdd71f4df594d662d8cd17c016",
            "66b7662b9efc4f84b7f7e074e706c7c8",
            "44c1753f9de5493dadc4965e8b429b34",
            "cb19a0a0635c49d199d14ea645f17c9d",
            "ae72a5cd64cd440dab446443d496f80c",
            "c5121fdfe5284a2da5d5625358597fb1",
            "a68613a4d65148299cacad52afc9ad7d",
            "86c0ec412f194580a49a842f697f8b19",
            "17a835a165b147d7ba316963ff83dd2f",
            "83c511f88e454bcca15ab7fd5c41716c"
          ]
        },
        "outputId": "8194a154-fb65-4519-db7f-6a0ac76151d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration limsc--concept-recognition-not-iob-49ad4a4453826183\n",
            "Reusing dataset parquet (/root/.cache/huggingface/datasets/limsc___parquet/limsc--concept-recognition-not-iob-49ad4a4453826183/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45eca55ace8445cd95985153d1282f51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'concept_tags'],\n",
              "        num_rows: 611\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'concept_tags'],\n",
              "        num_rows: 132\n",
              "    })\n",
              "    val: Dataset({\n",
              "        features: ['id', 'tokens', 'concept_tags'],\n",
              "        num_rows: 131\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds_name = 'limsc/concept-recognition-not-iob'\n",
        "ds = load_dataset(ds_name)\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Vn4qqTzmWqDN",
        "outputId": "64dead53-679a-45dc-85ef-c398d21ce1a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Materials / EEEs',\n",
              " 'O',\n",
              " 'GN&C',\n",
              " 'Thermal',\n",
              " 'Parameter',\n",
              " 'Quality control',\n",
              " 'Safety / Risk (Control)',\n",
              " 'System engineering',\n",
              " 'Space Environment',\n",
              " 'Cleanliness',\n",
              " 'Measurement',\n",
              " 'Telecom.',\n",
              " 'Project Organisation / Documentation',\n",
              " 'Project Scope',\n",
              " 'Power',\n",
              " 'OBDH',\n",
              " 'Structure & Mechanism',\n",
              " 'Nonconformity',\n",
              " 'Propulsion']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "label_list = ds[\"train\"].features['concept_tags'].feature.names\n",
        "label_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = 'bert-base-cased'"
      ],
      "metadata": {
        "id": "h81se9qE9F8X"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "prefix_space = True if model_type_dict[model_checkpoint] == 'roberta-base' else False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_type_dict[model_checkpoint],\n",
        "    use_fast = True,\n",
        "    add_prefix_space = prefix_space\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wGZn8Er5WqDQ"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples, label_all_tokens = False):\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples['tokens'],\n",
        "        truncation = True,\n",
        "        is_split_into_words = True\n",
        "    )\n",
        "    labels = []\n",
        "    \n",
        "    for i, label in enumerate(examples['concept_tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        \n",
        "        for word_idx in word_ids:           \n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            \n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            \n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            \n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    \n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "53730728-d258-4be1-c865-a10de17ffd25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "c4bbb2d5be9348838536264dde9bbc09",
            "c3127ea530394cd297743051bacc58b1",
            "dae8f53f25ef4e869f0e33e04c16a231",
            "df615bcbe8364dd186227caf73de332d",
            "b9c09810a82549608587502a718fab4a",
            "f4766bade6284a4885c651c459cb7aee",
            "f2cecd48008d4341be612efe1a851073",
            "4ec28348555c4c4080cca6439c7b3b77",
            "afd214d72d8d465b9f4afe6a0ed98200",
            "865add225923456cab12e4c66fbb13f7",
            "5c6cbbc072594327a395e791ae0fb992"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/limsc___parquet/limsc--concept-recognition-not-iob-49ad4a4453826183/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-fa1580622d14d025.arrow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4bbb2d5be9348838536264dde9bbc09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/limsc___parquet/limsc--concept-recognition-not-iob-49ad4a4453826183/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-f76fabb751962790.arrow\n"
          ]
        }
      ],
      "source": [
        "tokenized_ds = ds.map(tokenize_and_align_labels, batched = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Fine-tuning (Single Loop)**"
      ],
      "metadata": {
        "id": "wKmQPYsTQqyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "caHE49prWqDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db04c49-471c-4370-8fee-9d317d21d6e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:707: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  tensor = as_tensor(value)\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(\n",
        "    tokenizer = tokenizer,\n",
        "    padding = True,\n",
        "    return_tensors = 'tf'\n",
        ")\n",
        "\n",
        "def batching(tokenized_ds, batch_size):\n",
        "\n",
        "    batched_train_ds = tokenized_ds['train'].to_tf_dataset(\n",
        "        columns = ['attention_mask', 'input_ids', 'labels'],\n",
        "        shuffle = True,\n",
        "        batch_size = batch_size,\n",
        "        collate_fn = data_collator,\n",
        "    )\n",
        "\n",
        "    batched_val_ds = tokenized_ds['val'].to_tf_dataset(\n",
        "        columns = ['attention_mask', 'input_ids', 'labels'],\n",
        "        shuffle = False,\n",
        "        batch_size = batch_size,\n",
        "        collate_fn = data_collator,\n",
        "    )\n",
        "\n",
        "    batched_test_ds = tokenized_ds['test'].to_tf_dataset(\n",
        "        columns = ['attention_mask', 'input_ids', 'labels'],\n",
        "        shuffle = False,\n",
        "        batch_size = batch_size,\n",
        "        collate_fn = data_collator,\n",
        "    )\n",
        "    \n",
        "    return batched_train_ds, batched_val_ds, batched_test_ds\n",
        "\n",
        "batched_train_ds, batched_val_ds, batched_test_ds = batching(tokenized_ds, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForTokenClassification, create_optimizer\n",
        "\n",
        "seed = 6789767\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "num_epochs = 5\n",
        "initial_lr = 2e-5\n",
        "\n",
        "def create_model(num_epochs, initial_lr):\n",
        "\n",
        "    model = TFAutoModelForTokenClassification.from_pretrained(\n",
        "        model_checkpoint,\n",
        "        num_labels = len(label_list)\n",
        "    )\n",
        "\n",
        "    batches_per_epoch = len(tokenized_ds['train']) // batch_size\n",
        "    total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "\n",
        "    optimizer, schedule = create_optimizer(\n",
        "        init_lr = initial_lr,\n",
        "        num_warmup_steps = total_train_steps // 20,\n",
        "        num_train_steps = total_train_steps,\n",
        "        weight_decay_rate = 0.01\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer = optimizer)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model(num_epochs, initial_lr)"
      ],
      "metadata": {
        "id": "P2JF0qbzcKAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3e3e45-9a15-4c64-eaf8-1d16f5187ef8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from transformers.keras_callbacks import KerasMetricCallback\n",
        "\n",
        "# metric = load_metric(\"seqeval\")\n",
        "# labels = [label_list[i] for i in example['concept_tags']]\n",
        "# metric.compute(predictions=[labels], references=[labels])\n",
        "\n",
        "# def compute_metrics(p):\n",
        "#     predictions, labels = p\n",
        "#     # predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "#     # Remove ignored index (special tokens)\n",
        "#     true_predictions = [\n",
        "#         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "#         for prediction, label in zip(predictions, labels)\n",
        "#     ]\n",
        "#     true_labels = [\n",
        "#         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "#         for prediction, label in zip(predictions, labels)\n",
        "#     ]\n",
        "\n",
        "#     results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "#     return {\n",
        "#         \"precision\": results[\"overall_precision\"],\n",
        "#         \"recall\": results[\"overall_recall\"],\n",
        "#         \"f1\": results[\"overall_f1\"],\n",
        "#         \"accuracy\": results[\"overall_accuracy\"],\n",
        "#     }\n",
        "\n",
        "# metric_callback = KerasMetricCallback(\n",
        "#     metric_fn=compute_metrics, eval_dataset=validation_set\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, CSVLogger\n",
        "\n",
        "class update_logger(Callback):\n",
        "\n",
        "    def __init__(self):    \n",
        "        super(update_logger, self).__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs = {}):\n",
        "        logs['seed'] = seed\n",
        "        logs['batch_size'] = batch_size\n",
        "        logs['learning_rate'] = initial_lr\n",
        "\n",
        "update_logger_cb = update_logger()\n",
        "\n",
        "csvlogger_file = f'{model_name_dict[model_checkpoint]}-{task_name_dict[ds_name]}.csv'\n",
        "csvlogger_cb = CSVLogger(csvlogger_file, append = True)\n",
        "\n",
        "callbacks = [update_logger_cb, csvlogger_cb]"
      ],
      "metadata": {
        "id": "wmjnGrL4Ln9t"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "J6fjhEr0WqDS"
      },
      "outputs": [],
      "source": [
        "# # model = create_model(num_epochs, initial_lr)\n",
        "\n",
        "# model.fit(\n",
        "#     batched_train_ds,\n",
        "#     validation_data = batched_val_ds,\n",
        "#     epochs = num_epochs,\n",
        "#     callbacks = callbacks\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperparameter tuning**"
      ],
      "metadata": {
        "id": "iPOAWvTPQznQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [8] if model_type_dict[model_checkpoint] == 'roberta-base' else [8, 16]\n",
        "initial_lrs = [5e-5, 3e-5, 2e-5]\n",
        "seeds = [21916, 25412, 56281, 61712, 30488,\n",
        "         28215, 78867, 87843, 67918, 93327,\n",
        "         95420, 11905, 86349, 12082, 81996]\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "\n",
        "    batched_train_ds, batched_val_ds, batched_test_ds = batching(tokenized_ds, batch_size)\n",
        "    \n",
        "    for initial_lr in initial_lrs:\n",
        "    \n",
        "        for seed in seeds:\n",
        "    \n",
        "            tf.random.set_seed(seed)\n",
        "            model = create_model(num_epochs, initial_lr)\n",
        "\n",
        "            csvlogger_file = f'{model_name_dict[model_checkpoint]}-{task_name_dict[ds_name]}.csv'\n",
        "            csvlogger_cb = CSVLogger(csvlogger_file, append = True)\n",
        "\n",
        "            callbacks = [update_logger_cb, csvlogger_cb]\n",
        "\n",
        "            model.fit(\n",
        "                batched_train_ds,\n",
        "                validation_data = batched_val_ds,\n",
        "                epochs = num_epochs,\n",
        "                callbacks = callbacks\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkb_UO8TQ0HR",
        "outputId": "102cedc5-e441-4f9c-f4fb-ba4ca0810744"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:707: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  tensor = as_tensor(value)\n",
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 201ms/step - loss: 1.3934 - val_loss: 0.8224 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 168ms/step - loss: 0.6133 - val_loss: 0.5671 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.3711 - val_loss: 0.5052 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 163ms/step - loss: 0.2369 - val_loss: 0.5099 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.1743 - val_loss: 0.5249 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 201ms/step - loss: 1.3973 - val_loss: 0.9366 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.6913 - val_loss: 0.6081 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.4025 - val_loss: 0.5586 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.2572 - val_loss: 0.5455 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.1870 - val_loss: 0.5467 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 31s 202ms/step - loss: 1.3920 - val_loss: 0.8548 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.6572 - val_loss: 0.5945 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.4051 - val_loss: 0.5353 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 164ms/step - loss: 0.2669 - val_loss: 0.5242 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.1988 - val_loss: 0.5303 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 199ms/step - loss: 1.3565 - val_loss: 0.8414 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.6254 - val_loss: 0.5783 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.3632 - val_loss: 0.5147 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 170ms/step - loss: 0.2390 - val_loss: 0.5151 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.1712 - val_loss: 0.5164 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 202ms/step - loss: 1.3434 - val_loss: 0.8634 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.6443 - val_loss: 0.5996 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 168ms/step - loss: 0.3768 - val_loss: 0.5071 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.2396 - val_loss: 0.5083 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.1732 - val_loss: 0.5212 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 31s 203ms/step - loss: 1.3734 - val_loss: 0.8050 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.6353 - val_loss: 0.6051 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.3739 - val_loss: 0.5214 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.2383 - val_loss: 0.5269 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.1738 - val_loss: 0.5222 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 201ms/step - loss: 1.3413 - val_loss: 0.8800 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 170ms/step - loss: 0.6484 - val_loss: 0.5964 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.3844 - val_loss: 0.5382 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.2486 - val_loss: 0.5264 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.1808 - val_loss: 0.5340 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 200ms/step - loss: 1.3403 - val_loss: 0.8435 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.6204 - val_loss: 0.5905 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.3817 - val_loss: 0.5358 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.2481 - val_loss: 0.5148 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.1859 - val_loss: 0.5284 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 31s 205ms/step - loss: 1.3710 - val_loss: 0.8334 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.6412 - val_loss: 0.6310 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.3860 - val_loss: 0.5198 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.2539 - val_loss: 0.5285 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.1839 - val_loss: 0.5367 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 200ms/step - loss: 1.4266 - val_loss: 0.8618 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.6539 - val_loss: 0.6019 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.3821 - val_loss: 0.5480 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 159ms/step - loss: 0.2439 - val_loss: 0.5271 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.1805 - val_loss: 0.5331 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 203ms/step - loss: 1.3160 - val_loss: 0.7994 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 162ms/step - loss: 0.6120 - val_loss: 0.5896 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.3689 - val_loss: 0.5439 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.2369 - val_loss: 0.5178 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 162ms/step - loss: 0.1764 - val_loss: 0.5235 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 204ms/step - loss: 1.3424 - val_loss: 0.8402 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.6402 - val_loss: 0.6449 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 164ms/step - loss: 0.3962 - val_loss: 0.5352 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.2526 - val_loss: 0.5351 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.1876 - val_loss: 0.5235 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 208ms/step - loss: 1.3014 - val_loss: 0.8699 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.6572 - val_loss: 0.6375 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.3863 - val_loss: 0.5283 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.2451 - val_loss: 0.5173 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.1825 - val_loss: 0.5258 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 210ms/step - loss: 1.3245 - val_loss: 0.8150 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.6338 - val_loss: 0.5706 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.3762 - val_loss: 0.5433 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.2426 - val_loss: 0.5284 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.1790 - val_loss: 0.5316 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 200ms/step - loss: 1.2865 - val_loss: 0.8331 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.6373 - val_loss: 0.6349 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.3847 - val_loss: 0.5365 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.2444 - val_loss: 0.5425 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.1788 - val_loss: 0.5620 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 31s 203ms/step - loss: 1.4469 - val_loss: 0.9212 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 168ms/step - loss: 0.7203 - val_loss: 0.6569 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.4866 - val_loss: 0.5549 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 171ms/step - loss: 0.3611 - val_loss: 0.5355 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.2966 - val_loss: 0.5367 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 199ms/step - loss: 1.4616 - val_loss: 0.9171 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.7241 - val_loss: 0.6401 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.4737 - val_loss: 0.5637 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.3436 - val_loss: 0.5483 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.2817 - val_loss: 0.5431 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 202ms/step - loss: 1.4602 - val_loss: 0.9354 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.7552 - val_loss: 0.7062 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.5142 - val_loss: 0.5859 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 163ms/step - loss: 0.3861 - val_loss: 0.5594 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.3166 - val_loss: 0.5554 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 203ms/step - loss: 1.4123 - val_loss: 0.9779 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 160ms/step - loss: 0.7454 - val_loss: 0.6545 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 168ms/step - loss: 0.4824 - val_loss: 0.5561 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.3481 - val_loss: 0.5303 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 161ms/step - loss: 0.2866 - val_loss: 0.5277 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 198ms/step - loss: 1.4564 - val_loss: 0.9667 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.7543 - val_loss: 0.6900 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.5038 - val_loss: 0.5993 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.3712 - val_loss: 0.5630 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.3051 - val_loss: 0.5523 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 29s 196ms/step - loss: 1.5129 - val_loss: 0.9606 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.7722 - val_loss: 0.6727 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.5132 - val_loss: 0.5619 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.3778 - val_loss: 0.5422 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.3071 - val_loss: 0.5403 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 202ms/step - loss: 1.4752 - val_loss: 0.9827 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 159ms/step - loss: 0.7707 - val_loss: 0.6809 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.5064 - val_loss: 0.5908 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.3737 - val_loss: 0.5576 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.3084 - val_loss: 0.5516 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 214ms/step - loss: 1.4705 - val_loss: 0.9558 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.7505 - val_loss: 0.7155 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.5182 - val_loss: 0.5973 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.3916 - val_loss: 0.5671 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.3253 - val_loss: 0.5597 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 211ms/step - loss: 1.4544 - val_loss: 0.9652 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 162ms/step - loss: 0.7816 - val_loss: 0.6811 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 168ms/step - loss: 0.5225 - val_loss: 0.5798 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.3857 - val_loss: 0.5610 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.3189 - val_loss: 0.5572 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 211ms/step - loss: 1.5049 - val_loss: 1.0099 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 168ms/step - loss: 0.7949 - val_loss: 0.7015 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 164ms/step - loss: 0.5300 - val_loss: 0.5989 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.3936 - val_loss: 0.5527 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.3168 - val_loss: 0.5516 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 209ms/step - loss: 1.4068 - val_loss: 0.9132 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 168ms/step - loss: 0.7308 - val_loss: 0.6696 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.4891 - val_loss: 0.5954 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.3650 - val_loss: 0.5563 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.3022 - val_loss: 0.5474 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 208ms/step - loss: 1.4339 - val_loss: 0.9387 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.7535 - val_loss: 0.6796 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.5116 - val_loss: 0.5788 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.3779 - val_loss: 0.5517 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 168ms/step - loss: 0.3127 - val_loss: 0.5444 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 210ms/step - loss: 1.4308 - val_loss: 0.9823 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 162ms/step - loss: 0.7818 - val_loss: 0.7195 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 164ms/step - loss: 0.5237 - val_loss: 0.5969 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 162ms/step - loss: 0.3944 - val_loss: 0.5716 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.3265 - val_loss: 0.5553 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 31s 203ms/step - loss: 1.3878 - val_loss: 0.9596 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 162ms/step - loss: 0.7469 - val_loss: 0.6549 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.5003 - val_loss: 0.5861 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 168ms/step - loss: 0.3694 - val_loss: 0.5482 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.3094 - val_loss: 0.5447 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 198ms/step - loss: 1.4422 - val_loss: 0.9136 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.7330 - val_loss: 0.6708 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.4928 - val_loss: 0.5936 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.3686 - val_loss: 0.5512 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.3056 - val_loss: 0.5467 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 200ms/step - loss: 1.5399 - val_loss: 1.0286 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 158ms/step - loss: 0.8493 - val_loss: 0.7581 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.6220 - val_loss: 0.6565 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 158ms/step - loss: 0.5032 - val_loss: 0.6112 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.4434 - val_loss: 0.6002 - seed: 21916.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 198ms/step - loss: 1.5785 - val_loss: 1.0496 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.8754 - val_loss: 0.7892 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.6377 - val_loss: 0.6527 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 159ms/step - loss: 0.5083 - val_loss: 0.6073 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 168ms/step - loss: 0.4402 - val_loss: 0.6063 - seed: 25412.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 196ms/step - loss: 1.6036 - val_loss: 1.0236 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.8674 - val_loss: 0.8015 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.6551 - val_loss: 0.6784 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.5302 - val_loss: 0.6338 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.4688 - val_loss: 0.6212 - seed: 56281.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 201ms/step - loss: 1.4928 - val_loss: 1.0487 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.8675 - val_loss: 0.7481 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 159ms/step - loss: 0.6170 - val_loss: 0.6333 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 161ms/step - loss: 0.4777 - val_loss: 0.5891 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.4143 - val_loss: 0.5805 - seed: 61712.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 199ms/step - loss: 1.5076 - val_loss: 1.0585 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.8823 - val_loss: 0.7758 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.6297 - val_loss: 0.6518 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.4943 - val_loss: 0.6035 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.4288 - val_loss: 0.5968 - seed: 30488.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 199ms/step - loss: 1.6249 - val_loss: 1.0412 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 171ms/step - loss: 0.8673 - val_loss: 0.7713 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.6321 - val_loss: 0.6511 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.5021 - val_loss: 0.5982 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.4320 - val_loss: 0.5889 - seed: 28215.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 200ms/step - loss: 1.5549 - val_loss: 1.0720 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.8783 - val_loss: 0.7851 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.6309 - val_loss: 0.6567 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.4960 - val_loss: 0.6074 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.4303 - val_loss: 0.5952 - seed: 78867.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 201ms/step - loss: 1.6366 - val_loss: 1.0358 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.8668 - val_loss: 0.7998 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.6340 - val_loss: 0.6899 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.5136 - val_loss: 0.6349 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.4523 - val_loss: 0.6258 - seed: 87843.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 201ms/step - loss: 1.5485 - val_loss: 1.0563 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 169ms/step - loss: 0.8816 - val_loss: 0.7870 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.6430 - val_loss: 0.6737 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.5197 - val_loss: 0.6319 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 168ms/step - loss: 0.4553 - val_loss: 0.6192 - seed: 67918.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 31s 206ms/step - loss: 1.5694 - val_loss: 1.0746 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.8959 - val_loss: 0.8056 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.6451 - val_loss: 0.6677 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 163ms/step - loss: 0.5121 - val_loss: 0.6322 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.4505 - val_loss: 0.6123 - seed: 93327.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 199ms/step - loss: 1.5692 - val_loss: 1.0250 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.8566 - val_loss: 0.7705 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.6232 - val_loss: 0.6580 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.4986 - val_loss: 0.6214 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.4399 - val_loss: 0.6054 - seed: 95420.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 31s 209ms/step - loss: 1.5227 - val_loss: 1.0428 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.8450 - val_loss: 0.7797 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.6256 - val_loss: 0.6737 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 166ms/step - loss: 0.5003 - val_loss: 0.6211 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 162ms/step - loss: 0.4368 - val_loss: 0.6094 - seed: 11905.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 211ms/step - loss: 1.5221 - val_loss: 1.0620 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.8946 - val_loss: 0.7892 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.6419 - val_loss: 0.6677 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 12s 164ms/step - loss: 0.5128 - val_loss: 0.6295 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.4487 - val_loss: 0.6112 - seed: 86349.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 211ms/step - loss: 1.5818 - val_loss: 1.0365 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.8723 - val_loss: 0.7620 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.6272 - val_loss: 0.6561 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 167ms/step - loss: 0.5015 - val_loss: 0.6135 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.4385 - val_loss: 0.5968 - seed: 12082.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 30s 203ms/step - loss: 1.6052 - val_loss: 1.0317 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 12s 165ms/step - loss: 0.8756 - val_loss: 0.7737 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 12s 163ms/step - loss: 0.6307 - val_loss: 0.6576 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.4987 - val_loss: 0.6224 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "76/76 [==============================] - 13s 165ms/step - loss: 0.4366 - val_loss: 0.6150 - seed: 81996.0000 - batch_size: 8.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 428ms/step - loss: 1.6118 - val_loss: 1.0057 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 373ms/step - loss: 0.8094 - val_loss: 0.7119 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.5481 - val_loss: 0.6028 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 337ms/step - loss: 0.3957 - val_loss: 0.5580 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 368ms/step - loss: 0.3234 - val_loss: 0.5506 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 421ms/step - loss: 1.6256 - val_loss: 1.0747 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 344ms/step - loss: 0.8796 - val_loss: 0.7625 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.5792 - val_loss: 0.6166 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 355ms/step - loss: 0.4105 - val_loss: 0.5649 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 360ms/step - loss: 0.3280 - val_loss: 0.5666 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 32s 430ms/step - loss: 1.6206 - val_loss: 1.0549 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.8602 - val_loss: 0.7539 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.6031 - val_loss: 0.6433 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 352ms/step - loss: 0.4556 - val_loss: 0.5925 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.3737 - val_loss: 0.5861 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 422ms/step - loss: 1.5470 - val_loss: 1.0511 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 340ms/step - loss: 0.8373 - val_loss: 0.7050 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 352ms/step - loss: 0.5360 - val_loss: 0.5998 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.3901 - val_loss: 0.5550 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 356ms/step - loss: 0.3137 - val_loss: 0.5419 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 415ms/step - loss: 1.5247 - val_loss: 1.0626 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 348ms/step - loss: 0.8517 - val_loss: 0.7327 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.5546 - val_loss: 0.5934 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.4022 - val_loss: 0.5736 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 341ms/step - loss: 0.3196 - val_loss: 0.5578 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 430ms/step - loss: 1.5954 - val_loss: 1.0301 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 351ms/step - loss: 0.8333 - val_loss: 0.7360 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 343ms/step - loss: 0.5726 - val_loss: 0.6045 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 349ms/step - loss: 0.4133 - val_loss: 0.5832 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 354ms/step - loss: 0.3330 - val_loss: 0.5649 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 412ms/step - loss: 1.5398 - val_loss: 1.0638 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 359ms/step - loss: 0.8897 - val_loss: 0.8191 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 374ms/step - loss: 0.5966 - val_loss: 0.6443 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.4280 - val_loss: 0.5951 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 358ms/step - loss: 0.3526 - val_loss: 0.5831 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 429ms/step - loss: 1.5480 - val_loss: 1.0257 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 353ms/step - loss: 0.8407 - val_loss: 0.7662 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.5884 - val_loss: 0.6503 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 356ms/step - loss: 0.4344 - val_loss: 0.5971 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 355ms/step - loss: 0.3610 - val_loss: 0.5947 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 422ms/step - loss: 1.5790 - val_loss: 1.0424 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 360ms/step - loss: 0.8491 - val_loss: 0.7700 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 356ms/step - loss: 0.5745 - val_loss: 0.6111 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 358ms/step - loss: 0.4181 - val_loss: 0.5693 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.3429 - val_loss: 0.5599 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 418ms/step - loss: 1.6322 - val_loss: 1.0745 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.9017 - val_loss: 0.7994 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 376ms/step - loss: 0.6264 - val_loss: 0.6650 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 334ms/step - loss: 0.4567 - val_loss: 0.6037 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 354ms/step - loss: 0.3688 - val_loss: 0.6011 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 428ms/step - loss: 1.5313 - val_loss: 1.0277 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 345ms/step - loss: 0.8376 - val_loss: 0.7471 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.5746 - val_loss: 0.6159 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 369ms/step - loss: 0.4141 - val_loss: 0.5733 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 334ms/step - loss: 0.3373 - val_loss: 0.5646 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 422ms/step - loss: 1.5859 - val_loss: 1.0545 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 345ms/step - loss: 0.8456 - val_loss: 0.7678 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.5791 - val_loss: 0.6353 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 356ms/step - loss: 0.4240 - val_loss: 0.5844 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 358ms/step - loss: 0.3439 - val_loss: 0.5710 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 414ms/step - loss: 1.4739 - val_loss: 1.0637 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.8658 - val_loss: 0.7676 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 358ms/step - loss: 0.5846 - val_loss: 0.6282 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.4256 - val_loss: 0.5780 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 345ms/step - loss: 0.3494 - val_loss: 0.5719 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            " 6/38 [===>..........................] - ETA: 6s - loss: 2.9276WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1105s vs `on_train_batch_end` time: 0.1159s). Check your callbacks.\n",
            "38/38 [==============================] - 30s 417ms/step - loss: 1.5371 - val_loss: 1.0702 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.8522 - val_loss: 0.7325 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 356ms/step - loss: 0.5709 - val_loss: 0.6132 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 0.4131 - val_loss: 0.5694 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 351ms/step - loss: 0.3380 - val_loss: 0.5578 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 432ms/step - loss: 1.4936 - val_loss: 1.0236 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.8327 - val_loss: 0.7418 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 338ms/step - loss: 0.5731 - val_loss: 0.6224 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 360ms/step - loss: 0.4181 - val_loss: 0.5899 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 342ms/step - loss: 0.3389 - val_loss: 0.5769 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 442ms/step - loss: 1.6765 - val_loss: 1.1097 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 356ms/step - loss: 0.9403 - val_loss: 0.8415 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 358ms/step - loss: 0.7038 - val_loss: 0.7030 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.5645 - val_loss: 0.6512 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 353ms/step - loss: 0.4952 - val_loss: 0.6372 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 423ms/step - loss: 1.6939 - val_loss: 1.1107 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 355ms/step - loss: 0.9914 - val_loss: 0.8992 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 351ms/step - loss: 0.7464 - val_loss: 0.7474 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 368ms/step - loss: 0.5968 - val_loss: 0.6788 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 341ms/step - loss: 0.5256 - val_loss: 0.6599 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 442ms/step - loss: 1.6890 - val_loss: 1.1088 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 368ms/step - loss: 0.9561 - val_loss: 0.8894 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 355ms/step - loss: 0.7308 - val_loss: 0.7429 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 355ms/step - loss: 0.6004 - val_loss: 0.6798 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.5370 - val_loss: 0.6634 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 437ms/step - loss: 1.6174 - val_loss: 1.1030 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 341ms/step - loss: 0.9854 - val_loss: 0.8867 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 362ms/step - loss: 0.7277 - val_loss: 0.7103 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 358ms/step - loss: 0.5684 - val_loss: 0.6577 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 0.4950 - val_loss: 0.6363 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 29s 403ms/step - loss: 1.6497 - val_loss: 1.1033 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 362ms/step - loss: 0.9695 - val_loss: 0.8767 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 351ms/step - loss: 0.7211 - val_loss: 0.7232 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 343ms/step - loss: 0.5725 - val_loss: 0.6621 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 369ms/step - loss: 0.4993 - val_loss: 0.6444 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 418ms/step - loss: 1.7355 - val_loss: 1.1147 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 358ms/step - loss: 0.9618 - val_loss: 0.8798 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 343ms/step - loss: 0.7339 - val_loss: 0.7317 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 345ms/step - loss: 0.5912 - val_loss: 0.6665 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 349ms/step - loss: 0.5182 - val_loss: 0.6453 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 441ms/step - loss: 1.6610 - val_loss: 1.1147 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 346ms/step - loss: 0.9802 - val_loss: 0.8704 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 358ms/step - loss: 0.7165 - val_loss: 0.7333 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 350ms/step - loss: 0.5651 - val_loss: 0.6646 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 353ms/step - loss: 0.4933 - val_loss: 0.6482 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 449ms/step - loss: 1.6787 - val_loss: 1.1061 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.9447 - val_loss: 0.8697 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 369ms/step - loss: 0.7137 - val_loss: 0.7294 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 370ms/step - loss: 0.5825 - val_loss: 0.6682 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 358ms/step - loss: 0.5134 - val_loss: 0.6561 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 424ms/step - loss: 1.6337 - val_loss: 1.1108 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 341ms/step - loss: 0.9883 - val_loss: 0.8768 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 355ms/step - loss: 0.7438 - val_loss: 0.7467 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 356ms/step - loss: 0.6031 - val_loss: 0.6837 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 345ms/step - loss: 0.5327 - val_loss: 0.6696 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 424ms/step - loss: 1.6907 - val_loss: 1.1188 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.9991 - val_loss: 0.9165 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 345ms/step - loss: 0.7612 - val_loss: 0.7658 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 355ms/step - loss: 0.6070 - val_loss: 0.6913 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 368ms/step - loss: 0.5315 - val_loss: 0.6714 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 420ms/step - loss: 1.6134 - val_loss: 1.0907 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 369ms/step - loss: 0.9487 - val_loss: 0.8688 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 355ms/step - loss: 0.7106 - val_loss: 0.7373 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.5753 - val_loss: 0.6711 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.5054 - val_loss: 0.6570 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 428ms/step - loss: 1.6610 - val_loss: 1.1105 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.9562 - val_loss: 0.8711 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 374ms/step - loss: 0.7221 - val_loss: 0.7427 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 353ms/step - loss: 0.5886 - val_loss: 0.6791 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 353ms/step - loss: 0.5202 - val_loss: 0.6579 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 418ms/step - loss: 1.6020 - val_loss: 1.1083 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 330ms/step - loss: 0.9901 - val_loss: 0.9132 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 351ms/step - loss: 0.7579 - val_loss: 0.7620 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 355ms/step - loss: 0.6115 - val_loss: 0.6958 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 344ms/step - loss: 0.5339 - val_loss: 0.6732 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 420ms/step - loss: 1.5925 - val_loss: 1.1119 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 354ms/step - loss: 0.9653 - val_loss: 0.8499 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 0.7155 - val_loss: 0.7024 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 354ms/step - loss: 0.5683 - val_loss: 0.6533 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 351ms/step - loss: 0.4978 - val_loss: 0.6301 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 408ms/step - loss: 1.6654 - val_loss: 1.0864 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.9618 - val_loss: 0.8617 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 356ms/step - loss: 0.7261 - val_loss: 0.7233 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.5796 - val_loss: 0.6732 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.5098 - val_loss: 0.6608 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 3.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 410ms/step - loss: 1.7737 - val_loss: 1.1437 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 334ms/step - loss: 1.0440 - val_loss: 0.9488 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 358ms/step - loss: 0.8274 - val_loss: 0.8175 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 330ms/step - loss: 0.7036 - val_loss: 0.7563 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 354ms/step - loss: 0.6441 - val_loss: 0.7352 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 427ms/step - loss: 1.8002 - val_loss: 1.1562 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 1.0708 - val_loss: 1.0006 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.8746 - val_loss: 0.8629 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 345ms/step - loss: 0.7459 - val_loss: 0.7883 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 344ms/step - loss: 0.6798 - val_loss: 0.7711 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 411ms/step - loss: 1.8523 - val_loss: 1.1452 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 360ms/step - loss: 1.0540 - val_loss: 0.9802 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 348ms/step - loss: 0.8606 - val_loss: 0.8528 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 0.7423 - val_loss: 0.7930 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 360ms/step - loss: 0.6827 - val_loss: 0.7745 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 428ms/step - loss: 1.6985 - val_loss: 1.1326 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 343ms/step - loss: 1.0579 - val_loss: 0.9970 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 331ms/step - loss: 0.8764 - val_loss: 0.8446 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 343ms/step - loss: 0.7238 - val_loss: 0.7643 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 354ms/step - loss: 0.6552 - val_loss: 0.7380 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 403ms/step - loss: 1.7071 - val_loss: 1.1390 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 1.0619 - val_loss: 0.9939 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 342ms/step - loss: 0.8763 - val_loss: 0.8563 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 346ms/step - loss: 0.7336 - val_loss: 0.7750 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 369ms/step - loss: 0.6664 - val_loss: 0.7576 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 432ms/step - loss: 1.8574 - val_loss: 1.1610 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 1.0551 - val_loss: 0.9808 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.8565 - val_loss: 0.8465 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 359ms/step - loss: 0.7277 - val_loss: 0.7723 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 362ms/step - loss: 0.6647 - val_loss: 0.7518 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 425ms/step - loss: 1.7703 - val_loss: 1.1430 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 1.0639 - val_loss: 1.0137 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.8930 - val_loss: 0.8738 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 364ms/step - loss: 0.7468 - val_loss: 0.8002 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.6801 - val_loss: 0.7813 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 413ms/step - loss: 1.8700 - val_loss: 1.1462 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 1.0483 - val_loss: 0.9916 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 374ms/step - loss: 0.8626 - val_loss: 0.8661 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 339ms/step - loss: 0.7332 - val_loss: 0.7997 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 341ms/step - loss: 0.6708 - val_loss: 0.7752 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 429ms/step - loss: 1.7603 - val_loss: 1.1537 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 1.0637 - val_loss: 0.9966 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.8695 - val_loss: 0.8493 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 356ms/step - loss: 0.7338 - val_loss: 0.7843 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 0.6755 - val_loss: 0.7638 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 435ms/step - loss: 1.7878 - val_loss: 1.1677 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 367ms/step - loss: 1.0862 - val_loss: 1.0118 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 360ms/step - loss: 0.8927 - val_loss: 0.8761 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 350ms/step - loss: 0.7538 - val_loss: 0.8058 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 342ms/step - loss: 0.6924 - val_loss: 0.7830 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 426ms/step - loss: 1.8068 - val_loss: 1.1369 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 360ms/step - loss: 1.0514 - val_loss: 0.9810 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 353ms/step - loss: 0.8627 - val_loss: 0.8483 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 357ms/step - loss: 0.7336 - val_loss: 0.7830 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 355ms/step - loss: 0.6691 - val_loss: 0.7625 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 467ms/step - loss: 1.7360 - val_loss: 1.1587 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 1.0461 - val_loss: 0.9686 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.8473 - val_loss: 0.8450 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 357ms/step - loss: 0.7211 - val_loss: 0.7807 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 356ms/step - loss: 0.6619 - val_loss: 0.7664 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 448ms/step - loss: 1.7338 - val_loss: 1.1478 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 358ms/step - loss: 1.0754 - val_loss: 1.0255 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.9006 - val_loss: 0.8863 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.7675 - val_loss: 0.8129 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 337ms/step - loss: 0.7015 - val_loss: 0.7848 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 457ms/step - loss: 1.8019 - val_loss: 1.1431 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 353ms/step - loss: 1.0693 - val_loss: 0.9854 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 362ms/step - loss: 0.8654 - val_loss: 0.8441 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 362ms/step - loss: 0.7307 - val_loss: 0.7663 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 342ms/step - loss: 0.6643 - val_loss: 0.7453 - seed: 12082.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 417ms/step - loss: 1.8246 - val_loss: 1.1301 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 355ms/step - loss: 1.0491 - val_loss: 0.9758 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 0.8521 - val_loss: 0.8437 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 355ms/step - loss: 0.7317 - val_loss: 0.7784 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 355ms/step - loss: 0.6668 - val_loss: 0.7638 - seed: 81996.0000 - batch_size: 16.0000 - learning_rate: 2.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate on test set**"
      ],
      "metadata": {
        "id": "Zpt3-j6pCDGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicts = [model.predict(batch) for batch in batched_test_ds]"
      ],
      "metadata": {
        "id": "WiH14zw_D__V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "all_preds = []\n",
        "all_trues = []\n",
        "\n",
        "for batch in batched_test_ds:\n",
        "\n",
        "    y_preds_logits = model.predict(batch)['logits']\n",
        "    y_preds = np.argmax(y_preds_logits, axis = 2)\n",
        "    y_trues = batch['labels']\n",
        "    p = y_preds, y_trues\n",
        "\n",
        "    all_pred = [\n",
        "        [label_list[p] for (p, l) in zip(y_pred, y_true) if l != -100]\n",
        "        for y_pred, y_true in zip(y_preds, y_trues)]\n",
        "    all_true = [\n",
        "        [label_list[l] for (p, l) in zip(y_pred, y_true) if l != -100]\n",
        "        for y_pred, y_true in zip(y_preds, y_trues)]\n",
        "    \n",
        "    all_preds.extend([item for sublist in all_pred for item in sublist])\n",
        "    all_trues.extend([item for sublist in all_true for item in sublist])\n",
        "    \n",
        "    # break"
      ],
      "metadata": {
        "id": "1rqBsfR4ETCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(all_trues, all_preds))\n",
        "cr = classification_report(all_trues, all_preds, output_dict = True)"
      ],
      "metadata": {
        "id": "yY9VTrwMHKmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(cr)"
      ],
      "metadata": {
        "id": "dTq-5y5lIL7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prediction Pipeline**"
      ],
      "metadata": {
        "id": "ya4eePzoB8n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TokenClassificationPipeline\n",
        "\n",
        "pipe = TokenClassificationPipeline(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    # aggregation_strategy = 'simple'\n",
        ")"
      ],
      "metadata": {
        "id": "kGyJ2dQvqqKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The Micro-VCM datasheet shall contain the product type.'\n",
        "text = 'Storage conditions shall prevent the degredation of the structure.'\n",
        "text = 'A record of the process data shall be part of the process procedure'\n",
        "text = 'In case of doubt, the internal NRB shall classify nonconformances as major.'\n",
        "text = 'The Reserved shall be an 8-bit field that is set to 0x00.'\n",
        "\n",
        "pipe(text)"
      ],
      "metadata": {
        "id": "nyT6fezQCAGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "t3_finetuning_seqlabel_cr",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "Zpt3-j6pCDGC",
        "ya4eePzoB8n4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45eca55ace8445cd95985153d1282f51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9224affdd71f4df594d662d8cd17c016",
              "IPY_MODEL_66b7662b9efc4f84b7f7e074e706c7c8",
              "IPY_MODEL_44c1753f9de5493dadc4965e8b429b34"
            ],
            "layout": "IPY_MODEL_cb19a0a0635c49d199d14ea645f17c9d"
          }
        },
        "9224affdd71f4df594d662d8cd17c016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae72a5cd64cd440dab446443d496f80c",
            "placeholder": "​",
            "style": "IPY_MODEL_c5121fdfe5284a2da5d5625358597fb1",
            "value": "100%"
          }
        },
        "66b7662b9efc4f84b7f7e074e706c7c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a68613a4d65148299cacad52afc9ad7d",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86c0ec412f194580a49a842f697f8b19",
            "value": 3
          }
        },
        "44c1753f9de5493dadc4965e8b429b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a835a165b147d7ba316963ff83dd2f",
            "placeholder": "​",
            "style": "IPY_MODEL_83c511f88e454bcca15ab7fd5c41716c",
            "value": " 3/3 [00:00&lt;00:00, 57.85it/s]"
          }
        },
        "cb19a0a0635c49d199d14ea645f17c9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae72a5cd64cd440dab446443d496f80c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5121fdfe5284a2da5d5625358597fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a68613a4d65148299cacad52afc9ad7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86c0ec412f194580a49a842f697f8b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17a835a165b147d7ba316963ff83dd2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83c511f88e454bcca15ab7fd5c41716c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4bbb2d5be9348838536264dde9bbc09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3127ea530394cd297743051bacc58b1",
              "IPY_MODEL_dae8f53f25ef4e869f0e33e04c16a231",
              "IPY_MODEL_df615bcbe8364dd186227caf73de332d"
            ],
            "layout": "IPY_MODEL_b9c09810a82549608587502a718fab4a"
          }
        },
        "c3127ea530394cd297743051bacc58b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4766bade6284a4885c651c459cb7aee",
            "placeholder": "​",
            "style": "IPY_MODEL_f2cecd48008d4341be612efe1a851073",
            "value": "100%"
          }
        },
        "dae8f53f25ef4e869f0e33e04c16a231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec28348555c4c4080cca6439c7b3b77",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afd214d72d8d465b9f4afe6a0ed98200",
            "value": 1
          }
        },
        "df615bcbe8364dd186227caf73de332d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_865add225923456cab12e4c66fbb13f7",
            "placeholder": "​",
            "style": "IPY_MODEL_5c6cbbc072594327a395e791ae0fb992",
            "value": " 1/1 [00:00&lt;00:00, 14.59ba/s]"
          }
        },
        "b9c09810a82549608587502a718fab4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4766bade6284a4885c651c459cb7aee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2cecd48008d4341be612efe1a851073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ec28348555c4c4080cca6439c7b3b77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afd214d72d8d465b9f4afe6a0ed98200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "865add225923456cab12e4c66fbb13f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c6cbbc072594327a395e791ae0fb992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}