{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/limshaocong/SysBERT/blob/main/t3_finetuning_seqlabel_cr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preliminaries**"
      ],
      "metadata": {
        "id": "b0LYLPA78zuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "! pip install --user datasets transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG-L9BEI8uie",
        "outputId": "3702f5ff-511e-4c4a-912c-724bb69d3ea4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-694f0c32-a1d1-c639-5e2e-f66268b8b899)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! huggingface-cli login\n",
        "# # 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"
      ],
      "metadata": {
        "id": "WPpnEQBc8vx7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "# **Import & Pre-process Data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_type_dict = {\n",
        "    'bert-base-cased' : 'bert-base-cased',\n",
        "    'roberta-base' : 'roberta-base',\n",
        "    'allenai/scibert_scivocab_cased' : 'allenai/scibert_scivocab_cased',\n",
        "    'limsc/reqbert-tapt-epoch29' : 'bert-base-cased', # preferred\n",
        "    'limsc/reqbert-tapt-epoch30' : 'bert-base-cased',\n",
        "    'limsc/reqroberta-tapt-epoch20' : 'roberta-base',\n",
        "    'limsc/reqroberta-tapt-epoch33' : 'roberta-base',\n",
        "    'limsc/reqroberta-tapt-epoch43' : 'roberta-base', # preferred\n",
        "    'limsc/reqroberta-tapt-epoch50' : 'roberta-base',\n",
        "    'limsc/reqscibert-tapt-epoch10' : 'allenai/scibert_scivocab_cased', # preferred\n",
        "    'limsc/reqscibert-tapt-epoch20' : 'allenai/scibert_scivocab_cased', # preferred\n",
        "    'limsc/reqscibert-tapt-epoch31' : 'allenai/scibert_scivocab_cased',\n",
        "    'limsc/reqscibert-tapt-epoch49' : 'allenai/scibert_scivocab_cased',\n",
        "}\n",
        "\n",
        "model_name_dict = {\n",
        "    'bert-base-cased' : 'bert',\n",
        "    'roberta-base' : 'roberta',\n",
        "    'allenai/scibert_scivocab_cased' : 'scibert',\n",
        "    'limsc/reqbert-tapt-epoch29' : 'reqbert-e29',\n",
        "    'limsc/reqbert-tapt-epoch30' : 'reqbert-e30',\n",
        "    'limsc/reqroberta-tapt-epoch20' : 'reqroberta-e20',\n",
        "    'limsc/reqroberta-tapt-epoch33' : 'reqroberta-e33',\n",
        "    'limsc/reqroberta-tapt-epoch43' : 'reqroberta-e43',\n",
        "    'limsc/reqroberta-tapt-epoch50' : 'reqroberta-e50',\n",
        "    'limsc/reqscibert-tapt-epoch10' : 'reqscibert-e10',\n",
        "    'limsc/reqscibert-tapt-epoch20' : 'reqscibert-e20',\n",
        "    'limsc/reqscibert-tapt-epoch31' : 'reqscibert-e31',\n",
        "    'limsc/reqscibert-tapt-epoch49' : 'reqscibert-e49',\n",
        "}\n",
        "\n",
        "task_name_dict = {\n",
        "    'limsc/fr-nfr-classification' : 'frnfr',\n",
        "    'limsc/nfr-subclass-classification' : 'subclass',\n",
        "    'limsc/concept-recognition' : 'cr',\n",
        "    'limsc/concept-recognition-not-iob' : 'cr',\n",
        "    'limsc/sysmlv2-entity-extraction' : 'ee'\n",
        "}"
      ],
      "metadata": {
        "id": "HNVQoJ1CKvSb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IreSlFmlIrIm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "45eca55ace8445cd95985153d1282f51",
            "9224affdd71f4df594d662d8cd17c016",
            "66b7662b9efc4f84b7f7e074e706c7c8",
            "44c1753f9de5493dadc4965e8b429b34",
            "cb19a0a0635c49d199d14ea645f17c9d",
            "ae72a5cd64cd440dab446443d496f80c",
            "c5121fdfe5284a2da5d5625358597fb1",
            "a68613a4d65148299cacad52afc9ad7d",
            "86c0ec412f194580a49a842f697f8b19",
            "17a835a165b147d7ba316963ff83dd2f",
            "83c511f88e454bcca15ab7fd5c41716c"
          ]
        },
        "outputId": "8194a154-fb65-4519-db7f-6a0ac76151d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration limsc--concept-recognition-not-iob-49ad4a4453826183\n",
            "Reusing dataset parquet (/root/.cache/huggingface/datasets/limsc___parquet/limsc--concept-recognition-not-iob-49ad4a4453826183/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45eca55ace8445cd95985153d1282f51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'concept_tags'],\n",
              "        num_rows: 611\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'concept_tags'],\n",
              "        num_rows: 132\n",
              "    })\n",
              "    val: Dataset({\n",
              "        features: ['id', 'tokens', 'concept_tags'],\n",
              "        num_rows: 131\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds_name = 'limsc/concept-recognition-not-iob'\n",
        "ds = load_dataset(ds_name)\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Vn4qqTzmWqDN",
        "outputId": "64dead53-679a-45dc-85ef-c398d21ce1a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Materials / EEEs',\n",
              " 'O',\n",
              " 'GN&C',\n",
              " 'Thermal',\n",
              " 'Parameter',\n",
              " 'Quality control',\n",
              " 'Safety / Risk (Control)',\n",
              " 'System engineering',\n",
              " 'Space Environment',\n",
              " 'Cleanliness',\n",
              " 'Measurement',\n",
              " 'Telecom.',\n",
              " 'Project Organisation / Documentation',\n",
              " 'Project Scope',\n",
              " 'Power',\n",
              " 'OBDH',\n",
              " 'Structure & Mechanism',\n",
              " 'Nonconformity',\n",
              " 'Propulsion']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "label_list = ds[\"train\"].features['concept_tags'].feature.names\n",
        "label_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = 'bert-base-cased'"
      ],
      "metadata": {
        "id": "h81se9qE9F8X"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "prefix_space = True if model_type_dict[model_checkpoint] == 'roberta-base' else False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_type_dict[model_checkpoint],\n",
        "    use_fast = True,\n",
        "    add_prefix_space = prefix_space\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wGZn8Er5WqDQ"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples, label_all_tokens = False):\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples['tokens'],\n",
        "        truncation = True,\n",
        "        is_split_into_words = True\n",
        "    )\n",
        "    labels = []\n",
        "    \n",
        "    for i, label in enumerate(examples['concept_tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        \n",
        "        for word_idx in word_ids:           \n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            \n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            \n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            \n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    \n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "53730728-d258-4be1-c865-a10de17ffd25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "c4bbb2d5be9348838536264dde9bbc09",
            "c3127ea530394cd297743051bacc58b1",
            "dae8f53f25ef4e869f0e33e04c16a231",
            "df615bcbe8364dd186227caf73de332d",
            "b9c09810a82549608587502a718fab4a",
            "f4766bade6284a4885c651c459cb7aee",
            "f2cecd48008d4341be612efe1a851073",
            "4ec28348555c4c4080cca6439c7b3b77",
            "afd214d72d8d465b9f4afe6a0ed98200",
            "865add225923456cab12e4c66fbb13f7",
            "5c6cbbc072594327a395e791ae0fb992"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/limsc___parquet/limsc--concept-recognition-not-iob-49ad4a4453826183/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-fa1580622d14d025.arrow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4bbb2d5be9348838536264dde9bbc09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/limsc___parquet/limsc--concept-recognition-not-iob-49ad4a4453826183/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-f76fabb751962790.arrow\n"
          ]
        }
      ],
      "source": [
        "tokenized_ds = ds.map(tokenize_and_align_labels, batched = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Fine-tuning (Single Loop)**"
      ],
      "metadata": {
        "id": "wKmQPYsTQqyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "caHE49prWqDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db04c49-471c-4370-8fee-9d317d21d6e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:707: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  tensor = as_tensor(value)\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(\n",
        "    tokenizer = tokenizer,\n",
        "    padding = True,\n",
        "    return_tensors = 'tf'\n",
        ")\n",
        "\n",
        "def batching(tokenized_ds, batch_size):\n",
        "\n",
        "    batched_train_ds = tokenized_ds['train'].to_tf_dataset(\n",
        "        columns = ['attention_mask', 'input_ids', 'labels'],\n",
        "        shuffle = True,\n",
        "        batch_size = batch_size,\n",
        "        collate_fn = data_collator,\n",
        "    )\n",
        "\n",
        "    batched_val_ds = tokenized_ds['val'].to_tf_dataset(\n",
        "        columns = ['attention_mask', 'input_ids', 'labels'],\n",
        "        shuffle = False,\n",
        "        batch_size = batch_size,\n",
        "        collate_fn = data_collator,\n",
        "    )\n",
        "\n",
        "    batched_test_ds = tokenized_ds['test'].to_tf_dataset(\n",
        "        columns = ['attention_mask', 'input_ids', 'labels'],\n",
        "        shuffle = False,\n",
        "        batch_size = batch_size,\n",
        "        collate_fn = data_collator,\n",
        "    )\n",
        "    \n",
        "    return batched_train_ds, batched_val_ds, batched_test_ds\n",
        "\n",
        "batched_train_ds, batched_val_ds, batched_test_ds = batching(tokenized_ds, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForTokenClassification, create_optimizer\n",
        "\n",
        "seed = 6789767\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "num_epochs = 5\n",
        "initial_lr = 2e-5\n",
        "\n",
        "def create_model(num_epochs, initial_lr):\n",
        "\n",
        "    model = TFAutoModelForTokenClassification.from_pretrained(\n",
        "        model_checkpoint,\n",
        "        num_labels = len(label_list)\n",
        "    )\n",
        "\n",
        "    batches_per_epoch = len(tokenized_ds['train']) // batch_size\n",
        "    total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "\n",
        "    optimizer, schedule = create_optimizer(\n",
        "        init_lr = initial_lr,\n",
        "        num_warmup_steps = total_train_steps // 20,\n",
        "        num_train_steps = total_train_steps,\n",
        "        weight_decay_rate = 0.01\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer = optimizer)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model(num_epochs, initial_lr)"
      ],
      "metadata": {
        "id": "P2JF0qbzcKAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3e3e45-9a15-4c64-eaf8-1d16f5187ef8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from transformers.keras_callbacks import KerasMetricCallback\n",
        "\n",
        "# metric = load_metric(\"seqeval\")\n",
        "# labels = [label_list[i] for i in example['concept_tags']]\n",
        "# metric.compute(predictions=[labels], references=[labels])\n",
        "\n",
        "# def compute_metrics(p):\n",
        "#     predictions, labels = p\n",
        "#     # predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "#     # Remove ignored index (special tokens)\n",
        "#     true_predictions = [\n",
        "#         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "#         for prediction, label in zip(predictions, labels)\n",
        "#     ]\n",
        "#     true_labels = [\n",
        "#         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "#         for prediction, label in zip(predictions, labels)\n",
        "#     ]\n",
        "\n",
        "#     results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "#     return {\n",
        "#         \"precision\": results[\"overall_precision\"],\n",
        "#         \"recall\": results[\"overall_recall\"],\n",
        "#         \"f1\": results[\"overall_f1\"],\n",
        "#         \"accuracy\": results[\"overall_accuracy\"],\n",
        "#     }\n",
        "\n",
        "# metric_callback = KerasMetricCallback(\n",
        "#     metric_fn=compute_metrics, eval_dataset=validation_set\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, CSVLogger\n",
        "\n",
        "class update_logger(Callback):\n",
        "\n",
        "    def __init__(self):    \n",
        "        super(update_logger, self).__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs = {}):\n",
        "        logs['seed'] = seed\n",
        "        logs['batch_size'] = batch_size\n",
        "        logs['learning_rate'] = initial_lr\n",
        "\n",
        "update_logger_cb = update_logger()\n",
        "\n",
        "csvlogger_file = f'{model_name_dict[model_checkpoint]}-{task_name_dict[ds_name]}.csv'\n",
        "csvlogger_cb = CSVLogger(csvlogger_file, append = True)\n",
        "\n",
        "callbacks = [update_logger_cb, csvlogger_cb]"
      ],
      "metadata": {
        "id": "wmjnGrL4Ln9t"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "J6fjhEr0WqDS"
      },
      "outputs": [],
      "source": [
        "# # model = create_model(num_epochs, initial_lr)\n",
        "\n",
        "# model.fit(\n",
        "#     batched_train_ds,\n",
        "#     validation_data = batched_val_ds,\n",
        "#     epochs = num_epochs,\n",
        "#     callbacks = callbacks\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperparameter tuning**"
      ],
      "metadata": {
        "id": "iPOAWvTPQznQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [8] if model_type_dict[model_checkpoint] == 'roberta-base' else [8, 16]\n",
        "initial_lrs = [5e-5, 3e-5, 2e-5]\n",
        "seeds = [21916, 25412, 56281, 61712, 30488,\n",
        "         28215, 78867, 87843, 67918, 93327,\n",
        "         95420, 11905, 86349, 12082, 81996]\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "\n",
        "    batched_train_ds, batched_val_ds, batched_test_ds = batching(tokenized_ds, batch_size)\n",
        "    \n",
        "    for initial_lr in initial_lrs:\n",
        "    \n",
        "        for seed in seeds:\n",
        "    \n",
        "            tf.random.set_seed(seed)\n",
        "            model = create_model(num_epochs, initial_lr)\n",
        "\n",
        "            csvlogger_file = f'{model_name_dict[model_checkpoint]}-{task_name_dict[ds_name]}.csv'\n",
        "            csvlogger_cb = CSVLogger(csvlogger_file, append = True)\n",
        "\n",
        "            callbacks = [update_logger_cb, csvlogger_cb]\n",
        "\n",
        "            model.fit(\n",
        "                batched_train_ds,\n",
        "                validation_data = batched_val_ds,\n",
        "                epochs = num_epochs,\n",
        "                callbacks = callbacks\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkb_UO8TQ0HR",
        "outputId": "0ee8b082-6e5d-4b2a-cecf-332debda9999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:707: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  tensor = as_tensor(value)\n",
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 32s 420ms/step - loss: 1.5507 - val_loss: 1.0243 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 338ms/step - loss: 0.8092 - val_loss: 0.7070 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 351ms/step - loss: 0.5470 - val_loss: 0.5956 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 346ms/step - loss: 0.3993 - val_loss: 0.5652 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 350ms/step - loss: 0.3295 - val_loss: 0.5636 - seed: 21916.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 428ms/step - loss: 1.5623 - val_loss: 1.0586 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 330ms/step - loss: 0.8883 - val_loss: 0.7904 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 346ms/step - loss: 0.6084 - val_loss: 0.6475 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 356ms/step - loss: 0.4497 - val_loss: 0.5980 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 354ms/step - loss: 0.3628 - val_loss: 0.5839 - seed: 25412.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 446ms/step - loss: 1.5663 - val_loss: 1.0202 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 366ms/step - loss: 0.8529 - val_loss: 0.7598 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 349ms/step - loss: 0.5929 - val_loss: 0.6276 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 360ms/step - loss: 0.4459 - val_loss: 0.5853 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 345ms/step - loss: 0.3685 - val_loss: 0.5772 - seed: 56281.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 30s 425ms/step - loss: 1.5035 - val_loss: 1.0343 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.8286 - val_loss: 0.7022 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.5330 - val_loss: 0.5776 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 343ms/step - loss: 0.3781 - val_loss: 0.5640 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 377ms/step - loss: 0.3045 - val_loss: 0.5459 - seed: 61712.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 448ms/step - loss: 1.5068 - val_loss: 1.0626 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 0.8346 - val_loss: 0.7191 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 372ms/step - loss: 0.5522 - val_loss: 0.5977 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 0.3974 - val_loss: 0.5624 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 348ms/step - loss: 0.3238 - val_loss: 0.5596 - seed: 30488.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 422ms/step - loss: 1.6011 - val_loss: 1.0480 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 360ms/step - loss: 0.8540 - val_loss: 0.7531 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 363ms/step - loss: 0.5816 - val_loss: 0.6174 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 350ms/step - loss: 0.4182 - val_loss: 0.5711 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 335ms/step - loss: 0.3392 - val_loss: 0.5625 - seed: 28215.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 32s 438ms/step - loss: 1.5618 - val_loss: 1.0585 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 351ms/step - loss: 0.8658 - val_loss: 0.7547 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 350ms/step - loss: 0.5745 - val_loss: 0.6224 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 348ms/step - loss: 0.4138 - val_loss: 0.5889 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 353ms/step - loss: 0.3355 - val_loss: 0.5730 - seed: 78867.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 32s 435ms/step - loss: 1.5712 - val_loss: 1.0285 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 350ms/step - loss: 0.8264 - val_loss: 0.7513 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.5715 - val_loss: 0.6231 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 361ms/step - loss: 0.4220 - val_loss: 0.5948 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 14s 359ms/step - loss: 0.3464 - val_loss: 0.5829 - seed: 87843.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 415ms/step - loss: 1.5086 - val_loss: 1.0509 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 14s 358ms/step - loss: 0.8543 - val_loss: 0.7564 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 13s 346ms/step - loss: 0.5798 - val_loss: 0.6298 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 368ms/step - loss: 0.4292 - val_loss: 0.5802 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 356ms/step - loss: 0.3494 - val_loss: 0.5685 - seed: 67918.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 32s 431ms/step - loss: 1.5269 - val_loss: 1.0696 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 339ms/step - loss: 0.8785 - val_loss: 0.7899 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 365ms/step - loss: 0.6001 - val_loss: 0.6445 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 351ms/step - loss: 0.4318 - val_loss: 0.5746 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 330ms/step - loss: 0.3523 - val_loss: 0.5696 - seed: 93327.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 433ms/step - loss: 1.5657 - val_loss: 1.0232 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 351ms/step - loss: 0.8343 - val_loss: 0.7315 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 358ms/step - loss: 0.5620 - val_loss: 0.5987 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 356ms/step - loss: 0.4024 - val_loss: 0.5696 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 354ms/step - loss: 0.3271 - val_loss: 0.5574 - seed: 95420.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 31s 428ms/step - loss: 1.5074 - val_loss: 1.0131 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.8285 - val_loss: 0.7299 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 357ms/step - loss: 0.5624 - val_loss: 0.6225 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 14s 354ms/step - loss: 0.4062 - val_loss: 0.5753 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 13s 349ms/step - loss: 0.3361 - val_loss: 0.5632 - seed: 11905.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "38/38 [==============================] - 32s 457ms/step - loss: 1.5309 - val_loss: 1.0742 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 2/5\n",
            "38/38 [==============================] - 13s 347ms/step - loss: 0.8876 - val_loss: 0.7756 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 14s 355ms/step - loss: 0.6006 - val_loss: 0.6418 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 13s 352ms/step - loss: 0.4437 - val_loss: 0.6019 - seed: 86349.0000 - batch_size: 16.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 5/5\n",
            "33/38 [=========================>....] - ETA: 1s - loss: 0.3572"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate on test set**"
      ],
      "metadata": {
        "id": "Zpt3-j6pCDGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicts = [model.predict(batch) for batch in batched_test_ds]"
      ],
      "metadata": {
        "id": "WiH14zw_D__V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "all_preds = []\n",
        "all_trues = []\n",
        "\n",
        "for batch in batched_test_ds:\n",
        "\n",
        "    y_preds_logits = model.predict(batch)['logits']\n",
        "    y_preds = np.argmax(y_preds_logits, axis = 2)\n",
        "    y_trues = batch['labels']\n",
        "    p = y_preds, y_trues\n",
        "\n",
        "    all_pred = [\n",
        "        [label_list[p] for (p, l) in zip(y_pred, y_true) if l != -100]\n",
        "        for y_pred, y_true in zip(y_preds, y_trues)]\n",
        "    all_true = [\n",
        "        [label_list[l] for (p, l) in zip(y_pred, y_true) if l != -100]\n",
        "        for y_pred, y_true in zip(y_preds, y_trues)]\n",
        "    \n",
        "    all_preds.extend([item for sublist in all_pred for item in sublist])\n",
        "    all_trues.extend([item for sublist in all_true for item in sublist])\n",
        "    \n",
        "    # break"
      ],
      "metadata": {
        "id": "1rqBsfR4ETCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(all_trues, all_preds))\n",
        "cr = classification_report(all_trues, all_preds, output_dict = True)"
      ],
      "metadata": {
        "id": "yY9VTrwMHKmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(cr)"
      ],
      "metadata": {
        "id": "dTq-5y5lIL7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prediction Pipeline**"
      ],
      "metadata": {
        "id": "ya4eePzoB8n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TokenClassificationPipeline\n",
        "\n",
        "pipe = TokenClassificationPipeline(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    # aggregation_strategy = 'simple'\n",
        ")"
      ],
      "metadata": {
        "id": "kGyJ2dQvqqKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The Micro-VCM datasheet shall contain the product type.'\n",
        "text = 'Storage conditions shall prevent the degredation of the structure.'\n",
        "text = 'A record of the process data shall be part of the process procedure'\n",
        "text = 'In case of doubt, the internal NRB shall classify nonconformances as major.'\n",
        "text = 'The Reserved shall be an 8-bit field that is set to 0x00.'\n",
        "\n",
        "pipe(text)"
      ],
      "metadata": {
        "id": "nyT6fezQCAGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "t3_finetuning_seqlabel_cr",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "Zpt3-j6pCDGC",
        "ya4eePzoB8n4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45eca55ace8445cd95985153d1282f51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9224affdd71f4df594d662d8cd17c016",
              "IPY_MODEL_66b7662b9efc4f84b7f7e074e706c7c8",
              "IPY_MODEL_44c1753f9de5493dadc4965e8b429b34"
            ],
            "layout": "IPY_MODEL_cb19a0a0635c49d199d14ea645f17c9d"
          }
        },
        "9224affdd71f4df594d662d8cd17c016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae72a5cd64cd440dab446443d496f80c",
            "placeholder": "",
            "style": "IPY_MODEL_c5121fdfe5284a2da5d5625358597fb1",
            "value": "100%"
          }
        },
        "66b7662b9efc4f84b7f7e074e706c7c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a68613a4d65148299cacad52afc9ad7d",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86c0ec412f194580a49a842f697f8b19",
            "value": 3
          }
        },
        "44c1753f9de5493dadc4965e8b429b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a835a165b147d7ba316963ff83dd2f",
            "placeholder": "",
            "style": "IPY_MODEL_83c511f88e454bcca15ab7fd5c41716c",
            "value": " 3/3 [00:00&lt;00:00, 57.85it/s]"
          }
        },
        "cb19a0a0635c49d199d14ea645f17c9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae72a5cd64cd440dab446443d496f80c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5121fdfe5284a2da5d5625358597fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a68613a4d65148299cacad52afc9ad7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86c0ec412f194580a49a842f697f8b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17a835a165b147d7ba316963ff83dd2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83c511f88e454bcca15ab7fd5c41716c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4bbb2d5be9348838536264dde9bbc09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3127ea530394cd297743051bacc58b1",
              "IPY_MODEL_dae8f53f25ef4e869f0e33e04c16a231",
              "IPY_MODEL_df615bcbe8364dd186227caf73de332d"
            ],
            "layout": "IPY_MODEL_b9c09810a82549608587502a718fab4a"
          }
        },
        "c3127ea530394cd297743051bacc58b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4766bade6284a4885c651c459cb7aee",
            "placeholder": "",
            "style": "IPY_MODEL_f2cecd48008d4341be612efe1a851073",
            "value": "100%"
          }
        },
        "dae8f53f25ef4e869f0e33e04c16a231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec28348555c4c4080cca6439c7b3b77",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afd214d72d8d465b9f4afe6a0ed98200",
            "value": 1
          }
        },
        "df615bcbe8364dd186227caf73de332d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_865add225923456cab12e4c66fbb13f7",
            "placeholder": "",
            "style": "IPY_MODEL_5c6cbbc072594327a395e791ae0fb992",
            "value": " 1/1 [00:00&lt;00:00, 14.59ba/s]"
          }
        },
        "b9c09810a82549608587502a718fab4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4766bade6284a4885c651c459cb7aee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2cecd48008d4341be612efe1a851073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ec28348555c4c4080cca6439c7b3b77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afd214d72d8d465b9f4afe6a0ed98200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "865add225923456cab12e4c66fbb13f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c6cbbc072594327a395e791ae0fb992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}